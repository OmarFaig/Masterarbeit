{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:55:15.145100Z",
     "start_time": "2024-03-31T12:55:11.449947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from model import seedformer_dim128\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import open3d as o3d\n",
    "import os\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import utils.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset and Dataloader\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "Below is the implementation of custom dataset/dataloader. Additionally there is a collate function. The number of points in eaach pointcloud is \n",
    "different. In order to make them equal collate function is defined."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "717e2fba24cdcbb8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RacingDataset(Dataset):\n",
    "    def __init__(self,root_dir,target_points=2990):#4731\n",
    "        self.root_dir = root_dir\n",
    "        self.file_list = os.listdir(root_dir)\n",
    "        self.filter_file_list = self.filter_list()\n",
    "        self.target_points = target_points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filter_file_list)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        pcd_path = os.path.join(self.root_dir,self.filter_file_list[index])\n",
    "        pcd = o3d.io.read_point_cloud(pcd_path)\n",
    "\n",
    "        points = torch.tensor(pcd.points, dtype=torch.float32)\n",
    "\n",
    "        return points,pcd_path\n",
    "\n",
    "   \n",
    "    def filter_list(self):\n",
    "        '''\n",
    "        Filter the inputs so that only pcds with more than 50 points are included in the training\n",
    "        :return:\n",
    "        '''\n",
    "        filtered_list=[]\n",
    "        for filename in self.file_list:\n",
    "            pcd = o3d.io.read_point_cloud(os.path.join(self.root_dir,filename))\n",
    "            points = torch.tensor(pcd.points, dtype=torch.float32)\n",
    "            if len(points)>=0:\n",
    "                filtered_list.append(filename)\n",
    "        return filtered_list\n",
    "   \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:55:15.152080Z",
     "start_time": "2024-03-31T12:55:15.146831Z"
    }
   },
   "id": "18531201d491e956",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7716/474309544.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  pcd_pad_tens = torch.tensor(pcd_pad.points, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the largest point cloud\n",
    "pcd_pad = o3d.io.read_point_cloud(\"/home/omar/TUM/Data/cropped/sim/018840.pcd\")\n",
    "pcd_pad_tens = torch.tensor(pcd_pad.points, dtype=torch.float32)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = RacingDataset(root_dir=\"/home/omar/TUM/Data/cropped/real\", target_points=len(pcd_pad_tens))\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=8, collate_fn=utils.collate_fn)\n",
    "\n",
    "print(len(dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:55:16.999852Z",
     "start_time": "2024-03-31T12:55:15.153343Z"
    }
   },
   "id": "bb205b36c1489ab0",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:55:17.006730Z",
     "start_time": "2024-03-31T12:55:17.001548Z"
    }
   },
   "id": "ed0c8d51559b7f9b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:55:17.017271Z",
     "start_time": "2024-03-31T12:55:17.009198Z"
    }
   },
   "id": "f76a4d67ecfdff6c",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e28a0ccc7563064"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "No checkpoint found. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/1499 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 1/1499 [00:01<32:02,  1.28s/batch, loss=1.24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 2/1499 [00:01<20:23,  1.22batch/s, loss=3.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 3/1499 [00:02<17:09,  1.45batch/s, loss=7.34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 4/1499 [00:02<15:14,  1.64batch/s, loss=6.2] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 5/1499 [00:03<14:10,  1.76batch/s, loss=4.99]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 6/1499 [00:03<13:31,  1.84batch/s, loss=6.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 7/1499 [00:04<13:07,  1.89batch/s, loss=6.6] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 8/1499 [00:04<12:50,  1.94batch/s, loss=6.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 9/1499 [00:05<12:39,  1.96batch/s, loss=6.3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 10/1499 [00:05<12:31,  1.98batch/s, loss=5.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 11/1499 [00:06<12:25,  2.00batch/s, loss=5.31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 12/1499 [00:06<12:22,  2.00batch/s, loss=5.5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 13/1499 [00:07<12:19,  2.01batch/s, loss=8.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 14/1499 [00:07<12:18,  2.01batch/s, loss=9.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 15/1499 [00:08<12:16,  2.02batch/s, loss=9.85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 16/1499 [00:08<12:15,  2.02batch/s, loss=40.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 17/1499 [00:09<12:14,  2.02batch/s, loss=39.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|          | 18/1499 [00:09<12:14,  2.02batch/s, loss=37.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|▏         | 19/1499 [00:10<12:15,  2.01batch/s, loss=36]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|▏         | 20/1499 [00:10<12:13,  2.02batch/s, loss=34.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|▏         | 21/1499 [00:11<12:13,  2.02batch/s, loss=33.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   1%|▏         | 22/1499 [00:11<12:13,  2.01batch/s, loss=31.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   2%|▏         | 23/1499 [00:12<12:11,  2.02batch/s, loss=30.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   2%|▏         | 24/1499 [00:12<12:10,  2.02batch/s, loss=29.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   2%|▏         | 25/1499 [00:13<12:10,  2.02batch/s, loss=28.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   2%|▏         | 26/1499 [00:13<12:11,  2.01batch/s, loss=27.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   2%|▏         | 27/1499 [00:14<12:10,  2.01batch/s, loss=26.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   2%|▏         | 28/1499 [00:14<12:10,  2.01batch/s, loss=25.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   2%|▏         | 29/1499 [00:15<12:09,  2.01batch/s, loss=24.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   2%|▏         | 30/1499 [00:15<12:08,  2.02batch/s, loss=23.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len - pcd, torch.Size([4, 3, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   2%|▏         | 30/1499 [00:16<13:12,  1.85batch/s, loss=23.9]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 55\u001B[0m\n\u001B[1;32m     52\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     53\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m---> 55\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m     pbar\u001B[38;5;241m.\u001B[39mset_postfix(loss\u001B[38;5;241m=\u001B[39mrunning_loss \u001B[38;5;241m/\u001B[39m (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))  \u001B[38;5;66;03m# Update tqdm progress bar with the current loss\u001B[39;00m\n\u001B[1;32m     58\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep()  \u001B[38;5;66;03m# Step the learning rate scheduler\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "#del model\n",
    "model=seedformer_dim128(up_factors=[1, 2, 2])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "epochs=1\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size=15, gamma=0.1)  # Reduce LR by a factor of 0.1 every 30 epochs\n",
    "\n",
    "# Initialize variables for checkpointing\n",
    "#best_loss = float('inf')\n",
    "# Define paths for checkpoint and log file\n",
    "checkpoint_path = 'checkpoint_.pth'\n",
    "log_file = 'training_log.txt'\n",
    "\n",
    "# Check if a checkpoint exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_loss = checkpoint['loss']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    print(f\"Checkpoint loaded. Resuming training from epoch {start_epoch}. Best loss so far: {best_loss}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    # Wrap the DataLoader with tqdm to track progress\n",
    "    with tqdm(enumerate(dataloader, 0), total=len(dataloader), desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:\n",
    "        for i, data in pbar:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)  # Move data to GPU if available\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            losses = []\n",
    "\n",
    "            for input_pc, output_pc in zip(inputs, outputs):\n",
    "                # Calculate Chamfer Distance loss using pytorch3d.loss.chamfer_distance\n",
    "                loss, _ = chamfer_distance(input_pc.unsqueeze(0), output_pc.unsqueeze(0))\n",
    "                losses.append(loss)\n",
    "\n",
    "            loss = torch.mean(torch.stack(losses))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix(loss=running_loss / (i + 1))  # Update tqdm progress bar with the current loss\n",
    "\n",
    "        scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "    # Save checkpoint if current loss is the best seen so far\n",
    "    if running_loss < best_loss:\n",
    "        best_loss = running_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': running_loss\n",
    "        }, checkpoint_path)\n",
    " \n",
    "    # Log the epoch loss in the log file\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f'Epoch {epoch + 1} Loss: {running_loss / len(dataloader)}\\n')\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T12:55:33.941646Z",
     "start_time": "2024-03-31T12:55:17.018006Z"
    }
   },
   "id": "ca233b98555d8fc7",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save The Data\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bea2b4076db53cd4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#del model\n",
    "#import gc         # garbage collect library\n",
    "#gc.collect()\n",
    "#torch.cuda.empty_cache() "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f56b812c034053a0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "simulation_dataset = RacingDataset(root_dir=\"/home/omar/TUM/Data/cropped/sim\")\n",
    "simulation_dataloader = DataLoader(simulation_dataset, batch_size=1, shuffle=True, num_workers=8,collate_fn=utils.collate_fn)\n",
    "utils.apply_and_save_res(dataset=simulation_dataset,dataloader=simulation_dataloader,model=model,savedir=\"/home/omar/TUM/Data/reconstructed_cropped/sim_2803\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dc7f9fe7611c357",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#pcd=o3d.io.read_point_cloud(\"/home/omar/TUM/Data/reconstructed_cropped/sim_2803/018840.pcd\")\n",
    "#(\"/home/omar/TUM/data_MA/m1695833/Sim2RealDistributionAlignedDataset/sim/data/pcl/000035.pcd\")\n",
    "#print(len(pcd.points))\n",
    "pcl=np.load(\"/home/omar/TUM/Data/SeedFormer_2602_npy/reconstructed_2803/points/020100.npy\")\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(pcl)\n",
    "##box=[\"Car\" ,0.0, 0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,1.18, 1.9 ,4.88 ,1.56 ,6.55 ,1.01 ,-0.0]\n",
    "box=[\"Car\" ,0.0, 0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,1.18, 1.9 ,4.88 ,39.88 ,4.14 ,1.64 ,0.17]\n",
    "\n",
    "#pcd_cut=crop_invert_stitch(pcl,box)\n",
    "#pcd=pcd_cut+pcd_car_recons\n",
    "utils.visualize(pcl=pcd,bbox_coordinates=box)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d4e513c17deabb9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## \"Stitch\" the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6b07375205c94ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reconstructed_car_path=\"/home/omar/TUM/Data/reconstructed_cropped/sim_2803/\"\n",
    "original_sim_path=\"/home/omar/TUM/data_MA/m1695833/Sim2RealDistributionAlignedDataset/sim/data/pcl/\"\n",
    "bbox_path=\"/home/omar/TUM/data_MA/m1695833/Sim2RealDistributionAlignedDataset/sim/data/label\"\n",
    "#reconstructed_scene_path=\"/home/omar/TUM/Data/reconstructed_scene/sim/\"\n",
    "npy_final_path=\"/home/omar/TUM/Data/SeedFormer_2602_npy/reconstructed_2803/points/\"\n",
    "file_car_list=[]\n",
    "for filename_car in os.listdir(reconstructed_car_path):\n",
    "    file_car_list.append(filename_car)\n",
    "    txt=filename_car.replace('.pcd',\".txt\")\n",
    "    bbox = (open(bbox_path+\"/\"+txt, \"r\")).read().split()\n",
    "    original_pcd = o3d.io.read_point_cloud(original_sim_path+filename_car)\n",
    "    filename_=filename_car.split(\".\")[0]\n",
    "    #if  (int)(filename_)<=24995:#<>\n",
    "        \n",
    "        #print(filename_)\n",
    "    #np_array=np.asarray(original_pcd.points)#.pcd\n",
    "    #arr2d=np.asarray(pcd_sim.points)\n",
    "    #x=np_array.copy()[:,0]\n",
    "    #np_array[:,0]=np_array[:,2]\n",
    "    #np_array[:,2]=x\n",
    "   # np.save(npy_final_path+filename_,np_array)#.npy\n",
    "\n",
    "    reconstructed_car= o3d.io.read_point_cloud(reconstructed_car_path +\"/\"+ filename_car)\n",
    "    crop_invert=utils.crop_invert_stitch(original_pcd,reconstructed_car, bbox)\n",
    "    #o3d.io.write_point_cloud(reconstructed_scene_path+\"/\"+filename_car,crop_invert)\n",
    "    np_array=np.asarray(crop_invert.points)\n",
    "    np.save(npy_final_path+filename_,np_array)\n",
    "#"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb96cfc20a8049ea",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create and save the new dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5512f6b90a1a2862"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correcting Labels\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d239d3f92368e731"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "labels_folder=\"/home/omar/TUM/data_MA/m1695833/Sim2RealDistributionAlignedDataset/sim/data/label/\"\n",
    "save_folder=\"/home/omar/TUM/Data/SeedFormer_2602_npy/sim/labels/\"\n",
    "for label in os.listdir(labels_folder):\n",
    "    file = open(labels_folder+label)\n",
    "    bbox=file.read()\n",
    "    bbox_correct=utils.correct_bbox_label(bbox)\n",
    "    file.close()\n",
    "    file_write=open(save_folder+label,\"w+\")\n",
    "    file_write.write(\" \".join(bbox_correct))\n",
    "    file_write.close()\n",
    "    print(bbox_correct)\n",
    "    #print(items)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51ba0cfa6adeb93d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reconstructed_car_path=\"/home/omar/TUM/Data/combined/sim/\"\n",
    "#bbox_path=\"/home/omar/TUM/data_MA/m1695833/Sim2RealDistributionAlignedDataset/sim/data/label\"\n",
    "\n",
    "#original_sim_path=\"/home/omar/TUM/data_MA/m1695833/Sim2RealDistributionAlignedDataset/sim/data/pcl/\"\n",
    "original_sim_path=\"/home/omar/TUM/Data/reconstructed_scene/sim/\"\n",
    "#reconstructed_scene_path=\"/home/omar/TUM/Data/reconstructed_scene/sim/\"\n",
    "npy_final_path=\"/home/omar/TUM/Data/SeedFormer_2602_npy_zyx/reconstructed/points/\"\n",
    "file_car_list=[]\n",
    "for filename_car in os.listdir(reconstructed_car_path):\n",
    "    file_car_list.append(filename_car)\n",
    "    txt=filename_car.replace('.pcd',\".txt\")\n",
    "   # bbox = (open(bbox_path+\"/\"+txt, \"r\")).read().split()\n",
    "    original_pcd = o3d.io.read_point_cloud(original_sim_path+filename_car)\n",
    "    filename_=filename_car.split(\".\")[0]\n",
    "    #if  (int)(filename_)<=24995:#<>\n",
    "        \n",
    "        #print(filename_)\n",
    "    np_array=np.asarray(original_pcd.points)#.pcd\n",
    "    #arr2d=np.asarray(pcd_sim.points)\n",
    "    x=np_array.copy()[:,0]\n",
    "    np_array[:,0]=np_array[:,2]\n",
    "    np_array[:,2]=x\n",
    "    np.save(npy_final_path+filename_,np_array)#.npy\n",
    "\n",
    "    #reconstructed_car= o3d.io.read_point_cloud(reconstructed_car_path +\"/\"+ filename_car)\n",
    "    #crop_invert=crop_invert_stitch(original_pcd,reconstructed_car, bbox)\n",
    "    #o3d.io.write_point_cloud(reconstructed_scene_path+\"/\"+filename_car,crop_invert)\n",
    "    #np_array=np.asarray(crop_invert.points)\n",
    "    #np.save(npy_final_path+filename_car,np_array)\n",
    "#\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d38b9705c2507b2c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#source_label_path = \"/home/omar/TUM/Masterarbeit/Data/m1695833/Sim2RealDistributionAlignedDataset/real/data/label/\"\n",
    "#pcd_real=o3d.io.read_point_cloud(\"/home/omar/TUM/Data/cropped/sim/018840.pcd\")\n",
    "#pcd_sim=o3d.io.read_point_cloud(\"/home/omar/TUM/Data/reconstructed_scene/sim/018845.pcd\")\n",
    "#arr=np.asarray(pcd_sim.points)\n",
    "#print(pcd_sim.points)\n",
    "#print(\"np shape: \" ,np.asarray(pcd_sim.points).shape)\n",
    "#arr2d=np.asarray(pcd_sim.points)\n",
    "#x=arr2d.copy()[:,0]\n",
    "#arr2d[:,0]=arr2d[:,2]\n",
    "#arr2d[:,2]=x\n",
    "#pcd = o3d.geometry.PointCloud()\n",
    "#pcd.points = o3d.utility.Vector3dVector(arr2d)\n",
    "#o3d.io.write_point_cloud(\"/home/omar/TUM/Data/sync.pcd\", pcd)\n",
    "#\n",
    "## Load saved point cloud and visualize it\n",
    "#pcd_load = o3d.io.read_point_cloud(\"/home/omar/TUM/Data/sync.pcd\")\n",
    "##o3d.visualization.draw_geometries([pcd_load])\n",
    "#pcd_car_recons=o3d.io.read_point_cloud(\"/home/omar/TUM/Data/reconstructed_cropped/sim/020000.pcd\")\n",
    "#pcl=o3d.io.read_point_cloud(\"/home/omar/TUM/Data/SeedFormer_2602_npy_zyx/real/points/018840.npy\")\n",
    "pcl=np.load(\"/home/omar/TUM/Data/SeedFormer_2602_npy/reconstructed_2803/points/015500.npy\")\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(pcl)\n",
    "#box=[\"Car\" ,0.0, 0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,1.18, 1.9 ,4.88 ,1.56 ,6.55 ,1.01 ,-0.0]\n",
    "box=[\"Car\" ,0.0, 0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,1.18, 1.9 ,4.88 ,76.79 ,1.53,0.96 ,0.06]\n",
    "\n",
    "#pcd_cut=crop_invert_stitch(pcl,box)\n",
    "#pcd=pcd_cut+pcd_car_recons\n",
    "pcd=o3d.io.read_point_cloud()\n",
    "utils.visualize(pcl=pcd,bbox_coordinates=box)\n",
    "#utils.visualize(pcl=pcd_sim)\n",
    "#\n",
    "#utils.visualize(pcl=pcd_sim)\n",
    "#\n",
    "#utils.visualize(pcl=pcl)\n",
    "#\n",
    "#utils.visualize(pcl=pcd)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35be81a35c3b532b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#pcl=np.load(\"/home/omar/TUM/Data/SeedFormer_2602_npy/reconstructed/points/018840.npy\")\n",
    "pcd=o3d.io.read_point_cloud(\"/home/omar/TUM/Data/cropped/real/018840.pcd\")\n",
    "print(len(pcd.points))\n",
    "#pcd = o3d.geometry.PointCloud()\n",
    "#pcd.points = o3d.utility.Vector3dVector(pcl)\n",
    "#box=[\"Car\" ,0.0, 0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,1.18, 1.9 ,4.88 ,1.56 ,6.55 ,1.01 ,-0.0]\n",
    "box=[\"Car\" ,0.0, 0 ,0.0 ,0.0 ,0.0 ,0.0 ,0.0 ,1.18, 1.9 ,4.88 ,81.49 ,2.12,0.76 ,0.06]\n",
    "\n",
    "#pcd_cut=crop_invert_stitch(pcl,box)\n",
    "#pcd=pcd_cut+pcd_car_recons\n",
    "utils.visualize(pcl=pcd,bbox_coordinates=box)\n",
    "#utils.visualize(pcl=pcd_sim)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31785e53b0214b03",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "array2d=np.array([[1, 2, 3],[4, 5, 6],[7, 8, 9],[10, 11, 12]])\n",
    "print(\"array2d : \",array2d)\n",
    "print(\"array2d shape \" , array2d.shape)\n",
    "print(\"-------------\")\n",
    "#print(\"transpose : \" ,array2d.T)\n",
    "#print(\"reshape:  \" ,array2d.reshape(-1).T)\n",
    "x=array2d.copy()[:,0]\n",
    "print(\"x col : \" ,array2d[:,0])\n",
    "\n",
    "print(\"z col : \" ,array2d[:,2])\n",
    "array2d[:,0]=array2d[:,2]\n",
    "print(\"x\",x)\n",
    "array2d[:,2]=x\n",
    "print(\"x col  after swap: \" ,array2d)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a4fc9750f895a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a0d01b44f3f54d9e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cb64edf1b021b3cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load simulation dataset\n",
    "simulation_dataset = RacingDataset(root_dir=\"/home/omar/TUM/Data/cropped/sim\")\n",
    "simulation_dataloader = DataLoader(simulation_dataset, batch_size=1, shuffle=True, num_workers=8,collate_fn=collate_fn)\n",
    "\n",
    "# Load real dataset\n",
    "real_dataset = RacingDataset(root_dir=\"/home/omar/TUM/Data/cropped/real\")\n",
    "real_dataloader = DataLoader(real_dataset, batch_size=1, shuffle=True, num_workers=8,collate_fn=collate_fn)\n",
    "\n",
    "# Select a subset of samples\n",
    "num_samples = 3\n",
    "\n",
    "# Apply the model and visualize differences\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "for dataset, dataloader, name in [(simulation_dataset, simulation_dataloader, \"Simulation\"), \n",
    "                                  (real_dataset, real_dataloader, \"Real\")]:\n",
    "    for i, data in enumerate(tqdm(dataloader, desc=f'Processing {name} dataset', unit='point cloud')):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        \n",
    "        inputs, paths = data\n",
    "        #print(paths)\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        original_pc = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(inputs.squeeze().cpu().numpy()))\n",
    "        original_pc.paint_uniform_color([1, 0, 0])  # Paint original point cloud in red\n",
    "\n",
    "        reconstructed_pc = o3d.geometry.PointCloud(o3d.utility.Vector3dVector(outputs.squeeze().cpu().numpy()))\n",
    "        reconstructed_pc.paint_uniform_color([0, 1, 0])  # Paint reconstructed point cloud in green\n",
    "\n",
    "        o3d.visualization.draw_geometries([original_pc, reconstructed_pc], window_name=f\"{name} - Sample {i+1}\")\n",
    "#model=None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7b4375fbe9eb442",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Notes#\n",
    "'''\n",
    "input threshold adjust - \n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c4231e6d2e1734",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
